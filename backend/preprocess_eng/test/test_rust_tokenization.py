#!/usr/bin/env python3
"""
Quick test to verify Rust tokenization is enforced
Run this directly without starting the server
"""

import sys
import os

# Add backend to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'backend'))

def test_rust_enforcement():
    """Test that Rust tokenization is enforced"""
    print("=" * 80)
    print("RUST TOKENIZATION ENFORCEMENT TEST")
    print("=" * 80)
    print()
    
    try:
        # Test 1: Import check
        print("Test 1: Checking tokenizers package...")
        try:
            import tokenizers
            print("‚úÖ tokenizers package installed")
        except ImportError:
            print("‚ùå tokenizers package NOT installed")
            print("   Install with: pip install tokenizers")
            return False
        
        print()
        
        # Test 2: Force Rust initialization
        print("Test 2: Initializing tokenizer with force_rust=True...")
        from backend.preprocess_eng.tokenizer import Tokenizer
        
        try:
            tokenizer = Tokenizer(force_rust=True)
            print(f"‚úÖ Tokenizer initialized with method: {tokenizer.tokenizer_method}")
            
            if tokenizer.tokenizer_method == 'rust_tokenizers':
                print("‚úÖ ENFORCED: Using Rust-based tokenization")
            else:
                print(f"‚ùå FAILED: Using {tokenizer.tokenizer_method} instead of rust_tokenizers")
                return False
        except Exception as e:
            print(f"‚ùå FAILED: {e}")
            return False
        
        print()
        
        # Test 3: Performance test
        print("Test 3: Quick performance test...")
        import time
        
        test_query = "wireless headphones under 5000 rupees"
        
        # Warm up
        tokenizer.tokenize_step3_strict(test_query, tag_scripts=True)
        
        # Measure
        start = time.perf_counter()
        result = tokenizer.tokenize_step3_strict(test_query, tag_scripts=True)
        latency = (time.perf_counter() - start) * 1000
        
        print(f"   Query: {test_query}")
        print(f"   Tokens: {result['tokens']}")
        print(f"   Latency: {latency:.2f}ms")
        
        if latency < 3.0:
            print(f"‚úÖ Performance target met (<3ms)")
        else:
            print(f"‚ö†Ô∏è Latency higher than expected (target: <3ms, got: {latency:.2f}ms)")
        
        print()
        
        # Test 4: Try to create tokenizer without force_rust (should still use Rust)
        print("Test 4: Default initialization (should still prefer Rust)...")
        tokenizer2 = Tokenizer(force_rust=False)
        print(f"   Method: {tokenizer2.tokenizer_method}")
        
        if tokenizer2.tokenizer_method == 'rust_tokenizers':
            print("‚úÖ Default initialization uses Rust (as expected)")
        else:
            print(f"‚ö†Ô∏è Default initialization uses {tokenizer2.tokenizer_method} (PyICU fallback)")
        
        print()
        print("=" * 80)
        print("üéâ ALL TESTS PASSED!")
        print("=" * 80)
        print()
        print("Summary:")
        print("‚úÖ Rust tokenizers available and working")
        print("‚úÖ Enforcement mode working (force_rust=True)")
        print("‚úÖ Performance target met (<3ms)")
        print()
        
        return True
    
    except Exception as e:
        print()
        print("=" * 80)
        print("‚ùå TEST FAILED")
        print("=" * 80)
        print(f"Error: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = test_rust_enforcement()
    sys.exit(0 if success else 1)
